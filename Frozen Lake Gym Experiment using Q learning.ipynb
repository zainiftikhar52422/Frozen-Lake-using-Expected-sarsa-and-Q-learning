{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")           #Frozen lake env\n",
    "\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")        # if we want 8x8 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Q Learning Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning agent here\n",
    "class QLearningAgent():\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state,:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state, :]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward + (self.discount*np.max(current_q)))-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward)-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n                #number of actions in each state\n",
    "state_space_size = env.observation_space.n            #number of states\n",
    "# passing info to agent about number of actions and states\n",
    "agent_info = {\"num_actions\": action_space_size, \"num_states\": state_space_size, \"epsilon\": 0.1, \"step_size\": 0.01, \"discount\": 1, \"seed\": 0}\n",
    "current_agent = QLearningAgent()            #Using Q learning Agent\n",
    "current_agent.agent_init(agent_info)\n",
    "num_episodes=70000\n",
    "max_steps_per_episode=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S  See My Other project on github in which i use Expected Sarsa agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 70000/70000 [02:11<00:00, 532.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# joining Open AI GYM WITH Q LEARNING Agent\n",
    "rewards_all_episodes=list()\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    action=current_agent.agent_start(state)\n",
    "    new_state, reward, done, info=env.step(action)\n",
    "    state = new_state\n",
    "    rewards_current_episode += reward \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if(done == True):\n",
    "            current_agent.agent_end(reward)\n",
    "            break\n",
    "        else:\n",
    "            action=current_agent.agent_step(reward, state)\n",
    "            new_state, reward, done, info=env.step(action)\n",
    "            rewards_current_episode += reward \n",
    "            state=new_state\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.014000000000000005\n",
      "2000 :  0.017000000000000008\n",
      "3000 :  0.017000000000000008\n",
      "4000 :  0.016000000000000007\n",
      "5000 :  0.02000000000000001\n",
      "6000 :  0.016000000000000007\n",
      "7000 :  0.035000000000000024\n",
      "8000 :  0.02900000000000002\n",
      "9000 :  0.07200000000000005\n",
      "10000 :  0.07600000000000005\n",
      "11000 :  0.06200000000000005\n",
      "12000 :  0.06800000000000005\n",
      "13000 :  0.046000000000000034\n",
      "14000 :  0.06600000000000004\n",
      "15000 :  0.06600000000000004\n",
      "16000 :  0.07300000000000005\n",
      "17000 :  0.08900000000000007\n",
      "18000 :  0.060000000000000046\n",
      "19000 :  0.06700000000000005\n",
      "20000 :  0.08000000000000006\n",
      "21000 :  0.10200000000000008\n",
      "22000 :  0.10000000000000007\n",
      "23000 :  0.08900000000000007\n",
      "24000 :  0.08300000000000006\n",
      "25000 :  0.08600000000000006\n",
      "26000 :  0.10800000000000008\n",
      "27000 :  0.10100000000000008\n",
      "28000 :  0.08800000000000006\n",
      "29000 :  0.09600000000000007\n",
      "30000 :  0.12000000000000009\n",
      "31000 :  0.12800000000000009\n",
      "32000 :  0.11900000000000009\n",
      "33000 :  0.1210000000000001\n",
      "34000 :  0.1240000000000001\n",
      "35000 :  0.11400000000000009\n",
      "36000 :  0.11400000000000009\n",
      "37000 :  0.1410000000000001\n",
      "38000 :  0.11800000000000009\n",
      "39000 :  0.11800000000000009\n",
      "40000 :  0.1290000000000001\n",
      "41000 :  0.1420000000000001\n",
      "42000 :  0.1320000000000001\n",
      "43000 :  0.11700000000000009\n",
      "44000 :  0.1440000000000001\n",
      "45000 :  0.1370000000000001\n",
      "46000 :  0.25500000000000017\n",
      "47000 :  0.2740000000000002\n",
      "48000 :  0.23600000000000018\n",
      "49000 :  0.2740000000000002\n",
      "50000 :  0.3730000000000003\n",
      "51000 :  0.3810000000000003\n",
      "52000 :  0.3850000000000003\n",
      "53000 :  0.3840000000000003\n",
      "54000 :  0.3960000000000003\n",
      "55000 :  0.3930000000000003\n",
      "56000 :  0.3880000000000003\n",
      "57000 :  0.4210000000000003\n",
      "58000 :  0.3980000000000003\n",
      "59000 :  0.4240000000000003\n",
      "60000 :  0.3900000000000003\n",
      "61000 :  0.3950000000000003\n",
      "62000 :  0.3720000000000003\n",
      "63000 :  0.4070000000000003\n",
      "64000 :  0.3990000000000003\n",
      "65000 :  0.4090000000000003\n",
      "66000 :  0.3920000000000003\n",
      "67000 :  0.4250000000000003\n",
      "68000 :  0.4080000000000003\n",
      "69000 :  0.4010000000000003\n",
      "70000 :  0.4050000000000003\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.76131572 0.72154676 0.71523068 0.727399  ]\n",
      " [0.47640302 0.43534265 0.39271261 0.6534174 ]\n",
      " [0.53944246 0.40335159 0.36540187 0.45228239]\n",
      " [0.06581023 0.2274506  0.04938301 0.09341621]\n",
      " [0.7626714  0.50777255 0.50179235 0.51212646]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.350699   0.21193628 0.41917845 0.15153827]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51291181 0.50556576 0.46625105 0.76656245]\n",
      " [0.53596611 0.77567405 0.48044763 0.4922064 ]\n",
      " [0.71502504 0.54315217 0.42706361 0.36887915]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.50839219 0.55173777 0.83831227 0.54305716]\n",
      " [0.83800002 0.91975289 0.87921099 0.86324902]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(current_agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaving Using learned Target Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(current_agent.q[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking in How many Epsiodes our agent reaches goal using learned target policy using Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win 7323 Lose 2677\n"
     ]
    }
   ],
   "source": [
    "win=0\n",
    "lose=0\n",
    "for episode in range(10000):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    #time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        \n",
    "        action = np.argmax(current_agent.q[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            #clear_output(wait=True)\n",
    "            #env.render()\n",
    "            if reward == 1:\n",
    "                #print(\"****You reached the goal!****\")\n",
    "                #time.sleep(3)\n",
    "                win+=1\n",
    "            else:\n",
    "                #print(\"****You fell through a hole!****\")\n",
    "                #time.sleep(3)\n",
    "                #clear_output(wait=True)\n",
    "                lose+=1\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()\n",
    "print(\"win\",win,\"Lose\",lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
