{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import agent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")           #Frozen lake env\n",
    "\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")        # if we want 8x8 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Q Learning Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning agent here\n",
    "class QLearningAgent(agent.BaseAgent):\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state,:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state, :]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward + (self.discount*np.max(current_q)))-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward)-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n                #number of actions in each state\n",
    "state_space_size = env.observation_space.n            #number of states\n",
    "# passing info to agent about number of actions and states\n",
    "agent_info = {\"num_actions\": action_space_size, \"num_states\": state_space_size, \"epsilon\": 0.1, \"step_size\": 0.01, \"discount\": 1, \"seed\": 0}\n",
    "current_agent = QLearningAgent()            #Using Q learning Agent\n",
    "current_agent.agent_init(agent_info)\n",
    "num_episodes=70000\n",
    "max_steps_per_episode=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S  See My Other project on github in which i use Expected Sarsa agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 70000/70000 [03:03<00:00, 381.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# joining Open AI GYM WITH Q LEARNING Agent\n",
    "rewards_all_episodes=list()\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    action=current_agent.agent_start(state)\n",
    "    new_state, reward, done, info=env.step(action)\n",
    "    state = new_state\n",
    "    rewards_current_episode += reward \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if(done == True):\n",
    "            current_agent.agent_end(reward)\n",
    "            break\n",
    "        else:\n",
    "            action=current_agent.agent_step(reward, state)\n",
    "            new_state, reward, done, info=env.step(action)\n",
    "            rewards_current_episode += reward \n",
    "            state=new_state\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.05100000000000004\n",
      "2000 :  0.058000000000000045\n",
      "3000 :  0.04400000000000003\n",
      "4000 :  0.05600000000000004\n",
      "5000 :  0.047000000000000035\n",
      "6000 :  0.04300000000000003\n",
      "7000 :  0.057000000000000044\n",
      "8000 :  0.08700000000000006\n",
      "9000 :  0.09300000000000007\n",
      "10000 :  0.07100000000000005\n",
      "11000 :  0.11500000000000009\n",
      "12000 :  0.11000000000000008\n",
      "13000 :  0.1560000000000001\n",
      "14000 :  0.20600000000000016\n",
      "15000 :  0.23100000000000018\n",
      "16000 :  0.2970000000000002\n",
      "17000 :  0.3140000000000002\n",
      "18000 :  0.32400000000000023\n",
      "19000 :  0.3180000000000002\n",
      "20000 :  0.33100000000000024\n",
      "21000 :  0.4100000000000003\n",
      "22000 :  0.4280000000000003\n",
      "23000 :  0.4160000000000003\n",
      "24000 :  0.3800000000000003\n",
      "25000 :  0.3930000000000003\n",
      "26000 :  0.4040000000000003\n",
      "27000 :  0.4140000000000003\n",
      "28000 :  0.4110000000000003\n",
      "29000 :  0.4030000000000003\n",
      "30000 :  0.4030000000000003\n",
      "31000 :  0.3810000000000003\n",
      "32000 :  0.3980000000000003\n",
      "33000 :  0.4120000000000003\n",
      "34000 :  0.4170000000000003\n",
      "35000 :  0.4020000000000003\n",
      "36000 :  0.4200000000000003\n",
      "37000 :  0.4140000000000003\n",
      "38000 :  0.4170000000000003\n",
      "39000 :  0.4190000000000003\n",
      "40000 :  0.3980000000000003\n",
      "41000 :  0.4040000000000003\n",
      "42000 :  0.4140000000000003\n",
      "43000 :  0.43800000000000033\n",
      "44000 :  0.4050000000000003\n",
      "45000 :  0.3990000000000003\n",
      "46000 :  0.3950000000000003\n",
      "47000 :  0.4120000000000003\n",
      "48000 :  0.4180000000000003\n",
      "49000 :  0.3930000000000003\n",
      "50000 :  0.4120000000000003\n",
      "51000 :  0.4070000000000003\n",
      "52000 :  0.3970000000000003\n",
      "53000 :  0.3800000000000003\n",
      "54000 :  0.3860000000000003\n",
      "55000 :  0.4150000000000003\n",
      "56000 :  0.4010000000000003\n",
      "57000 :  0.3970000000000003\n",
      "58000 :  0.3950000000000003\n",
      "59000 :  0.44400000000000034\n",
      "60000 :  0.4070000000000003\n",
      "61000 :  0.44100000000000034\n",
      "62000 :  0.43400000000000033\n",
      "63000 :  0.3880000000000003\n",
      "64000 :  0.3980000000000003\n",
      "65000 :  0.3920000000000003\n",
      "66000 :  0.4050000000000003\n",
      "67000 :  0.3770000000000003\n",
      "68000 :  0.4060000000000003\n",
      "69000 :  0.4070000000000003\n",
      "70000 :  0.4180000000000003\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.40987954 0.38082047 0.38311356 0.38409398]\n",
      " [0.23310797 0.22937261 0.21953898 0.35671729]\n",
      " [0.29617451 0.28467442 0.27107701 0.30935657]\n",
      " [0.18684501 0.18040792 0.15595081 0.28666116]\n",
      " [0.42585448 0.30192443 0.28240295 0.25949214]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23414336 0.13958608 0.26565235 0.10661849]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3124551  0.31600368 0.31108977 0.46202819]\n",
      " [0.32874337 0.54247702 0.37009162 0.31296692]\n",
      " [0.56233744 0.4004389  0.36393346 0.25611765]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37873504 0.51490529 0.64749893 0.47313953]\n",
      " [0.66742334 0.80544676 0.7751423  0.70475864]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(current_agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaving Using learned Target Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(current_agent.q[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking in How many Epsiodes our agent reaches goal using learned target policy using Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win 7323 Lose 2677\n"
     ]
    }
   ],
   "source": [
    "win=0\n",
    "lose=0\n",
    "for episode in range(10000):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    #time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        \n",
    "        action = np.argmax(current_agent.q[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            #clear_output(wait=True)\n",
    "            #env.render()\n",
    "            if reward == 1:\n",
    "                #print(\"****You reached the goal!****\")\n",
    "                #time.sleep(3)\n",
    "                win+=1\n",
    "            else:\n",
    "                #print(\"****You fell through a hole!****\")\n",
    "                #time.sleep(3)\n",
    "                #clear_output(wait=True)\n",
    "                lose+=1\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()\n",
    "print(\"win\",win,\"Lose\",lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
