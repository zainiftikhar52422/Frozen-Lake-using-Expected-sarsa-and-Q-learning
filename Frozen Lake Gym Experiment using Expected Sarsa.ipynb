{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import agent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")           #Frozen lake env\n",
    "\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")        # if we want 8x8 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy Expected sarsa Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Sarsa agent here\n",
    "class ExpectedSarsaAgent(agent.BaseAgent):\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state, :]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state,:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update (~5 lines)\n",
    "        ### START CODE HERE ###\n",
    "        m = max(current_q)\n",
    "        greedies=[i for i, j in enumerate(current_q) if j == m]\n",
    "        probability= np.ones(self.num_actions) * (self.epsilon/self.num_actions)\n",
    "        for i in greedies:\n",
    "            probability[i] += ((1 - self.epsilon)/len(greedies))\n",
    "        expectation = np.sum(current_q * probability)\n",
    "        target = (self.discount*expectation) + reward\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * (target-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward)-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n                #number of actions in each state\n",
    "state_space_size = env.observation_space.n            #number of states\n",
    "# passing info to agent about number of actions and states\n",
    "agent_info = {\"num_actions\": action_space_size, \"num_states\": state_space_size, \"epsilon\": 0.1, \"step_size\": 0.01, \"discount\": 1, \"seed\": 0}\n",
    "current_agent = ExpectedSarsaAgent()            #Using Expected Sarsa Agent\n",
    "current_agent.agent_init(agent_info)\n",
    "num_episodes=70000\n",
    "max_steps_per_episode=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S  See My Other project on github in which i use Q Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 70000/70000 [02:49<00:00, 412.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# joining Open AI GYM WITH Expected Sarsa Agent\n",
    "rewards_all_episodes=list()\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    action=current_agent.agent_start(state)\n",
    "    new_state, reward, done, info=env.step(action)\n",
    "    state = new_state\n",
    "    rewards_current_episode += reward \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if(done == True):\n",
    "            current_agent.agent_end(reward)\n",
    "            break\n",
    "        else:\n",
    "            action=current_agent.agent_step(reward, state)\n",
    "            new_state, reward, done, info=env.step(action)\n",
    "            rewards_current_episode += reward \n",
    "            state=new_state\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.024000000000000014\n",
      "2000 :  0.02000000000000001\n",
      "3000 :  0.022000000000000013\n",
      "4000 :  0.03000000000000002\n",
      "5000 :  0.017000000000000008\n",
      "6000 :  0.022000000000000013\n",
      "7000 :  0.04400000000000003\n",
      "8000 :  0.04000000000000003\n",
      "9000 :  0.04900000000000004\n",
      "10000 :  0.06200000000000005\n",
      "11000 :  0.07100000000000005\n",
      "12000 :  0.08400000000000006\n",
      "13000 :  0.10400000000000008\n",
      "14000 :  0.11100000000000008\n",
      "15000 :  0.1580000000000001\n",
      "16000 :  0.18200000000000013\n",
      "17000 :  0.25100000000000017\n",
      "18000 :  0.33100000000000024\n",
      "19000 :  0.3100000000000002\n",
      "20000 :  0.3070000000000002\n",
      "21000 :  0.2930000000000002\n",
      "22000 :  0.2880000000000002\n",
      "23000 :  0.3170000000000002\n",
      "24000 :  0.32000000000000023\n",
      "25000 :  0.3980000000000003\n",
      "26000 :  0.4060000000000003\n",
      "27000 :  0.4290000000000003\n",
      "28000 :  0.3890000000000003\n",
      "29000 :  0.3960000000000003\n",
      "30000 :  0.3920000000000003\n",
      "31000 :  0.4110000000000003\n",
      "32000 :  0.3830000000000003\n",
      "33000 :  0.3910000000000003\n",
      "34000 :  0.36700000000000027\n",
      "35000 :  0.4020000000000003\n",
      "36000 :  0.3900000000000003\n",
      "37000 :  0.4050000000000003\n",
      "38000 :  0.3990000000000003\n",
      "39000 :  0.3850000000000003\n",
      "40000 :  0.4020000000000003\n",
      "41000 :  0.3890000000000003\n",
      "42000 :  0.4000000000000003\n",
      "43000 :  0.3780000000000003\n",
      "44000 :  0.4060000000000003\n",
      "45000 :  0.4130000000000003\n",
      "46000 :  0.4310000000000003\n",
      "47000 :  0.4090000000000003\n",
      "48000 :  0.3980000000000003\n",
      "49000 :  0.3900000000000003\n",
      "50000 :  0.4110000000000003\n",
      "51000 :  0.3940000000000003\n",
      "52000 :  0.3940000000000003\n",
      "53000 :  0.3870000000000003\n",
      "54000 :  0.4110000000000003\n",
      "55000 :  0.4030000000000003\n",
      "56000 :  0.3810000000000003\n",
      "57000 :  0.3810000000000003\n",
      "58000 :  0.3940000000000003\n",
      "59000 :  0.4120000000000003\n",
      "60000 :  0.3890000000000003\n",
      "61000 :  0.3800000000000003\n",
      "62000 :  0.3950000000000003\n",
      "63000 :  0.4160000000000003\n",
      "64000 :  0.3840000000000003\n",
      "65000 :  0.4240000000000003\n",
      "66000 :  0.3900000000000003\n",
      "67000 :  0.4200000000000003\n",
      "68000 :  0.3910000000000003\n",
      "69000 :  0.4050000000000003\n",
      "70000 :  0.4010000000000003\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.40228217 0.38379747 0.38270601 0.37872023]\n",
      " [0.24683992 0.22555915 0.19801417 0.34424902]\n",
      " [0.2926299  0.2309869  0.22349642 0.2480392 ]\n",
      " [0.1392673  0.06974012 0.04440527 0.08551391]\n",
      " [0.41937304 0.28388277 0.30385142 0.2738895 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25283855 0.22271445 0.22205421 0.07576774]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27866125 0.33258665 0.30776647 0.45723094]\n",
      " [0.34705839 0.5216258  0.4131865  0.33447838]\n",
      " [0.52206365 0.46661646 0.37114987 0.25553435]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37121508 0.50665051 0.65887584 0.448794  ]\n",
      " [0.66800414 0.82485241 0.78386452 0.73006513]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(current_agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaving Using learned Target Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "****You fell through a hole!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        # We use epsilon policy as it was our target policy\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = current_agent.q[state, :]\n",
    "        if current_agent.rand_generator.rand() < current_agent.epsilon:\n",
    "            action = current_agent.rand_generator.randint(current_agent.num_actions)\n",
    "        else:\n",
    "            action = current_agent.argmax(current_q)\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking in How many Epsiodes our agent reaches goal using learned target policy using Expected Sarsa algorthim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win 131 Lose 9869\n"
     ]
    }
   ],
   "source": [
    "win=0\n",
    "lose=0\n",
    "for episode in range(10000):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    #time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        \n",
    "        current_q = current_agent.q[state, :]\n",
    "        if current_agent.rand_generator.rand() < current_agent.epsilon:\n",
    "            action = current_agent.rand_generator.randint(current_agent.num_actions)\n",
    "        else:\n",
    "            action = current_agent.argmax(current_q)       \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                win+=1\n",
    "            else:\n",
    "                lose+=1\n",
    "            break\n",
    "env.close()\n",
    "print(\"win\",win,\"Lose\",lose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win times are very low because this env is very small and due to randomness in our policy and enviroment chances of felling into hole are greater as compare to Q learning agent in which Only randomness is in enviroment and not in target policy, So thats why in Q learning chances of falling down into hole are low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
