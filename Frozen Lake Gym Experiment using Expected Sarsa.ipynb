{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")           #Frozen lake env\n",
    "\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")        # if we want 8x8 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy Expected sarsa Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Sarsa agent here\n",
    "class ExpectedSarsaAgent():\n",
    "    def agent_init(self, agent_init_info):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n",
    "        {\n",
    "            num_states (int): The number of states,\n",
    "            num_actions (int): The number of actions,\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor,\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.num_states = agent_init_info[\"num_states\"]\n",
    "        self.epsilon = agent_init_info[\"epsilon\"]\n",
    "        self.step_size = agent_init_info[\"step_size\"]\n",
    "        self.discount = agent_init_info[\"discount\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state, :]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state,:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update (~5 lines)\n",
    "        ### START CODE HERE ###\n",
    "        m = max(current_q)\n",
    "        greedies=[i for i, j in enumerate(current_q) if j == m]\n",
    "        probability= np.ones(self.num_actions) * (self.epsilon/self.num_actions)\n",
    "        for i in greedies:\n",
    "            probability[i] += ((1 - self.epsilon)/len(greedies))\n",
    "        expectation = np.sum(current_q * probability)\n",
    "        target = (self.discount*expectation) + reward\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * (target-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode (1 line)\n",
    "        ### START CODE HERE ###\n",
    "        self.q[self.prev_state,self.prev_action]=self.q[self.prev_state,self.prev_action] + self.step_size * ((reward)-self.q[self.prev_state,self.prev_action])\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n                #number of actions in each state\n",
    "state_space_size = env.observation_space.n            #number of states\n",
    "# passing info to agent about number of actions and states\n",
    "agent_info = {\"num_actions\": action_space_size, \"num_states\": state_space_size, \"epsilon\": 0.1, \"step_size\": 0.01, \"discount\": 1, \"seed\": 0}\n",
    "current_agent = ExpectedSarsaAgent()            #Using Expected Sarsa Agent\n",
    "current_agent.agent_init(agent_info)\n",
    "num_episodes=70000\n",
    "max_steps_per_episode=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S  See My Other project on github in which i use Q Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 70000/70000 [03:20<00:00, 349.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# joining Open AI GYM WITH Expected Sarsa Agent\n",
    "rewards_all_episodes=list()\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    action=current_agent.agent_start(state)\n",
    "    new_state, reward, done, info=env.step(action)\n",
    "    state = new_state\n",
    "    rewards_current_episode += reward \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if(done == True):\n",
    "            current_agent.agent_end(reward)\n",
    "            break\n",
    "        else:\n",
    "            action=current_agent.agent_step(reward, state)\n",
    "            new_state, reward, done, info=env.step(action)\n",
    "            rewards_current_episode += reward \n",
    "            state=new_state\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.014000000000000005\n",
      "2000 :  0.03200000000000002\n",
      "3000 :  0.027000000000000017\n",
      "4000 :  0.03900000000000003\n",
      "5000 :  0.01900000000000001\n",
      "6000 :  0.027000000000000017\n",
      "7000 :  0.027000000000000017\n",
      "8000 :  0.02900000000000002\n",
      "9000 :  0.024000000000000014\n",
      "10000 :  0.03200000000000002\n",
      "11000 :  0.05000000000000004\n",
      "12000 :  0.09000000000000007\n",
      "13000 :  0.1300000000000001\n",
      "14000 :  0.1410000000000001\n",
      "15000 :  0.11000000000000008\n",
      "16000 :  0.1350000000000001\n",
      "17000 :  0.1490000000000001\n",
      "18000 :  0.18900000000000014\n",
      "19000 :  0.2700000000000002\n",
      "20000 :  0.2890000000000002\n",
      "21000 :  0.2660000000000002\n",
      "22000 :  0.32600000000000023\n",
      "23000 :  0.32600000000000023\n",
      "24000 :  0.33700000000000024\n",
      "25000 :  0.36100000000000027\n",
      "26000 :  0.3820000000000003\n",
      "27000 :  0.4120000000000003\n",
      "28000 :  0.4310000000000003\n",
      "29000 :  0.36300000000000027\n",
      "30000 :  0.4260000000000003\n",
      "31000 :  0.4000000000000003\n",
      "32000 :  0.3880000000000003\n",
      "33000 :  0.4230000000000003\n",
      "34000 :  0.4020000000000003\n",
      "35000 :  0.4150000000000003\n",
      "36000 :  0.4050000000000003\n",
      "37000 :  0.4120000000000003\n",
      "38000 :  0.4120000000000003\n",
      "39000 :  0.3970000000000003\n",
      "40000 :  0.4230000000000003\n",
      "41000 :  0.3920000000000003\n",
      "42000 :  0.4140000000000003\n",
      "43000 :  0.4160000000000003\n",
      "44000 :  0.3740000000000003\n",
      "45000 :  0.4120000000000003\n",
      "46000 :  0.4190000000000003\n",
      "47000 :  0.4100000000000003\n",
      "48000 :  0.4140000000000003\n",
      "49000 :  0.3870000000000003\n",
      "50000 :  0.4260000000000003\n",
      "51000 :  0.43200000000000033\n",
      "52000 :  0.4080000000000003\n",
      "53000 :  0.3980000000000003\n",
      "54000 :  0.3980000000000003\n",
      "55000 :  0.4060000000000003\n",
      "56000 :  0.4140000000000003\n",
      "57000 :  0.3980000000000003\n",
      "58000 :  0.4060000000000003\n",
      "59000 :  0.4180000000000003\n",
      "60000 :  0.4100000000000003\n",
      "61000 :  0.4150000000000003\n",
      "62000 :  0.3970000000000003\n",
      "63000 :  0.4190000000000003\n",
      "64000 :  0.3980000000000003\n",
      "65000 :  0.3920000000000003\n",
      "66000 :  0.3920000000000003\n",
      "67000 :  0.4120000000000003\n",
      "68000 :  0.4010000000000003\n",
      "69000 :  0.4090000000000003\n",
      "70000 :  0.4050000000000003\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.40573535 0.37931583 0.38140747 0.38189574]\n",
      " [0.24013068 0.23864755 0.20343636 0.34287952]\n",
      " [0.29060162 0.24287916 0.22488395 0.25940833]\n",
      " [0.0470688  0.13384151 0.01433711 0.06047199]\n",
      " [0.42131516 0.25488511 0.27849846 0.26688613]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25068357 0.1760117  0.24971406 0.09581088]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29864036 0.31530614 0.30526703 0.46227893]\n",
      " [0.34013479 0.5395687  0.39087278 0.33726455]\n",
      " [0.53085812 0.45345862 0.37811862 0.26299078]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38008471 0.50359384 0.66014389 0.43298138]\n",
      " [0.65301496 0.82964887 0.75595884 0.73397729]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(current_agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaving Using learned Target Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "****You fell through a hole!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        # We use epsilon policy as it was our target policy\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = current_agent.q[state, :]\n",
    "        if current_agent.rand_generator.rand() < current_agent.epsilon:\n",
    "            action = current_agent.rand_generator.randint(current_agent.num_actions)\n",
    "        else:\n",
    "            action = current_agent.argmax(current_q)\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        # Set new state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking in How many Epsiodes our agent reaches goal using learned target policy using Expected Sarsa algorthim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win 4008 Lose 5992\n"
     ]
    }
   ],
   "source": [
    "win=0\n",
    "lose=0\n",
    "for episode in range(10000):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    #print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    #time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        \n",
    "        current_q = current_agent.q[state, :]\n",
    "        if current_agent.rand_generator.rand() < current_agent.epsilon:\n",
    "            action = current_agent.rand_generator.randint(current_agent.num_actions)\n",
    "        else:\n",
    "            action = current_agent.argmax(current_q)       \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                win+=1\n",
    "            else:\n",
    "                lose+=1\n",
    "            break\n",
    "env.close()\n",
    "print(\"win\",win,\"Lose\",lose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Win times are very low because this env is very small and due to randomness in our policy and enviroment chances of felling into hole are greater as compare to Q learning agent in which Only randomness is in enviroment and not in target policy, So thats why in Q learning chances of falling down into hole are low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
